{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2255be7c-f6cb-451e-b5fd-1db1ea5a2e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments over FLCHAIN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import dataset_name\n",
    "print(f\"Experiments over {dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbec2c38-6d19-421e-a270-d77a130c14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"../../datasets/train/{dataset_name}.csv\")\n",
    "test = pd.read_csv(f\"../../datasets/test/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2903a1-16bb-4837-81d6-af2357444ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete instances with duration = 0.0\n",
    "idx = train[train[\"duration\"] == 0].index.values if (train[\"duration\"] == 0).any() else None\n",
    "if idx is not None:\n",
    "    train.drop(index=idx, inplace=True)\n",
    "idx = test[test[\"duration\"] == 0].index if (test[\"duration\"] == 0).any() else None\n",
    "if idx is not None:\n",
    "    test.drop(index=idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af100904-5dd8-4ccc-9428-d5e705a8442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE (5868, 10), TEST SHAPE (653, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"TRAIN SHAPE {train.shape}, TEST SHAPE {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70337be-5d68-41a4-ba5f-b2fe37108b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.drop([\"event\", \"duration\"], axis=1), train[[\"event\", \"duration\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fac4c66-5047-43b9-8d3c-5b87f70c225a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>sample.yr</th>\n",
       "      <th>kappa</th>\n",
       "      <th>lambda</th>\n",
       "      <th>flc.grp</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>mgus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>0.225</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1996</td>\n",
       "      <td>1.110</td>\n",
       "      <td>1.55</td>\n",
       "      <td>5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  sample.yr  kappa  lambda  flc.grp  creatinine  mgus\n",
       "0  60.0  0.0       1997  0.225    1.16        1         1.0   1.0\n",
       "1  80.0  0.0       1996  1.110    1.55        5         0.8   0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1850561f-05eb-42fd-bb0b-d312cbd0619b",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning\n",
    "### The `grid_params` dict describe all the possible hyperparameter values i chose to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94aca34f-5e4a-40ae-a416-f35562ac9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.models import DeepHitSingle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import KFold\n",
    "from pycox.evaluation import EvalSurv\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from torch.nn.modules.activation import ReLU, SELU\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "np.random.seed(42)\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df67b425-285b-4a9f-88b6-a1644c5a34c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "CPU times: user 1d 33min 34s, sys: 23h 19min 49s, total: 1d 23h 53min 23s\n",
      "Wall time: 1d 10h 24min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_name = \"deep_hit\"\n",
    "\n",
    "grid_params = {\n",
    "    # net params\n",
    "    \"dropout\": [0.3, 0.1],\n",
    "    \"num_nodes\": [[3, 5], [32, 32], [28,28,100,28,28], [16,32,64,64]],\n",
    "    \"activation\": [ReLU, SELU],\n",
    "    # loss params\n",
    "    \"alpha\": [0.8, 0.1],    \n",
    "    \"sigma\": [0.8, 0.1],    \n",
    "    # fit params\n",
    "    \"batch_size\": [50, 256],\n",
    "    \"epochs\": [256],\n",
    "    # optimizer params\n",
    "    \"lr\": [1e-2, 1e-4],\n",
    "    \"weight_decay\": [1e-2, 1e-3]\n",
    "}\n",
    "\n",
    "keys, values = zip(*grid_params.items())\n",
    "experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "for experiment in experiments:\n",
    "    Statistics = {'concordance_td':[], \n",
    "                  'ibs': [],\n",
    "                  'c_index': [],\n",
    "                  'avg_concordance_td':0.5, \n",
    "                  'avg_ibs': 0,\n",
    "                  'avg_c_index': 0.5,\n",
    "                  'std_concordance_td': 0,\n",
    "                  'std_ibs': 0,\n",
    "                  'std_c_index': 0                  \n",
    "                 }\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for i, (train_index, val_index) in enumerate(cv.split(X)):\n",
    "\n",
    "        # split to train and test \n",
    "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
    "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
    "\n",
    "        # preprocess for y\n",
    "        lower, upper = np.min(y_train[\"duration\"]), np.max(y_train[\"duration\"])\n",
    "        idx = np.where((y_val[\"duration\"] > lower) & (y_val[\"duration\"] < upper))[0]\n",
    "        X_val = X_val.iloc[idx].copy()\n",
    "        y_val = y_val.iloc[idx].copy()\n",
    "\n",
    "        original_y_train = y_train.copy()\n",
    "        original_y_val = y_val.copy()\n",
    "\n",
    "        # preprocess step\n",
    "        cols_standardize = ['age', 'sample.yr', 'kappa', 'lambda', 'flc.grp', 'creatinine']\n",
    "        cols_leave = ['sex', 'mgus']\n",
    "\n",
    "        standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "        leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "        standard_scaler = DataFrameMapper(standardize + leave)\n",
    "        X_train = standard_scaler.fit_transform(X_train).astype('float32')\n",
    "        y_train = y_train[\"duration\"].values, y_train[\"event\"].values\n",
    "        \n",
    "        X_val = standard_scaler.transform(X_val).astype('float32')\n",
    "        y_val = y_val[\"duration\"].values, y_val[\"event\"].values\n",
    "        \n",
    "        ## duration discretezation\n",
    "        tmp_times = np.sort(y_train[0])\n",
    "        times = np.array([tmp_times[i] for i in range(0, len(tmp_times), 10) if tmp_times[i] > 0], dtype='float32')\n",
    "        labtrans = DeepHitSingle.label_transform(times)\n",
    "        y_train = labtrans.transform(*y_train)\n",
    "        y_val = labtrans.transform(*y_val)      \n",
    "        val = X_val, y_val\n",
    "        \n",
    "        # model params\n",
    "        alpha = experiment[\"alpha\"]\n",
    "        sigma = experiment[\"sigma\"]        \n",
    "        \n",
    "        # build the network\n",
    "        in_features = X_train.shape[1]\n",
    "        num_nodes = experiment[\"num_nodes\"]\n",
    "        dropout = experiment[\"dropout\"]\n",
    "        activation = experiment[\"activation\"]\n",
    "        out_features = labtrans.out_features\n",
    "        batch_norm = True\n",
    "        output_bias = False\n",
    "        \n",
    "        net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
    "                              dropout, activation, output_bias=output_bias)\n",
    "\n",
    "        optimizer = tt.optim.Adam(lr = experiment[\"lr\"],\n",
    "                                  weight_decay = experiment[\"weight_decay\"])\n",
    "\n",
    "        # fit params\n",
    "        batch_size = experiment[\"batch_size\"]\n",
    "        epochs = experiment[\"epochs\"]\n",
    "        callbacks = [tt.callbacks.EarlyStopping()]\n",
    "        verbose = False\n",
    "\n",
    "        # train the model\n",
    "        model = DeepHitSingle(net, tt.optim.Adam, alpha=alpha, sigma=sigma, duration_index=labtrans.cuts)        \n",
    "        log = model.fit(X_train, y_train, batch_size, epochs, callbacks, verbose, val_data=val, val_batch_size=256)\n",
    "\n",
    "       \n",
    "        # model evaluation\n",
    "        ## pycox measures\n",
    "        estimate_surv = model.interpolate(10).predict_surv_df(X_val)\n",
    "        ev = EvalSurv(estimate_surv, y_val[0], y_val[1], censor_surv='km')\n",
    "\n",
    "        concordance_td = ev.concordance_td('antolini')\n",
    "        ibs = ev.integrated_brier_score(times)\n",
    "\n",
    "        ## lifelines measures\n",
    "        estimate = np.mean(1-estimate_surv, axis=0)\n",
    "        c_index = concordance_index(event_times=original_y_val[\"duration\"], \n",
    "                          predicted_scores= estimate, \n",
    "                          event_observed=original_y_val[\"event\"])\n",
    "\n",
    "        # store statistics in Statistics dict\n",
    "        Statistics[\"c_index\"].append(c_index)\n",
    "        Statistics[\"ibs\"].append(ibs)\n",
    "        Statistics[\"concordance_td\"].append(concordance_td)\n",
    "\n",
    "        \n",
    "    # summarise cross validation scores\n",
    "    Statistics[\"avg_concordance_td\"] = np.mean(Statistics[\"concordance_td\"])\n",
    "    Statistics[\"avg_ibs\"] = np.mean(Statistics[\"ibs\"])\n",
    "    Statistics[\"avg_c_index\"] = np.mean(Statistics[\"c_index\"])    \n",
    "    Statistics[\"std_concordance_td\"] = np.std(Statistics[\"concordance_td\"])\n",
    "    Statistics[\"std_ibs\"] = np.std(Statistics[\"ibs\"])    \n",
    "    Statistics[\"std_c_index\"] = np.std(Statistics[\"c_index\"])\n",
    "    \n",
    "   # save the model for later\n",
    "    try:\n",
    "        os.mkdir(f\"statistics/{model_name}/models\")\n",
    "\n",
    "    except OSError as error: \n",
    "        \n",
    "        file_name = f\"{model_name}_\"\n",
    "        for k,v in experiment.items():\n",
    "            file_name += f\"{str(k)}_{str(v)}_\"\n",
    "        \n",
    "        # dump model to files\n",
    "        path_sc = os.path.join(f\"statistics/{model_name}/models\", f\"sc_{file_name}.pkl\")            \n",
    "        path_net = os.path.join(f\"statistics/{model_name}/models\", f\"net_{file_name}\")\n",
    "        path_weights = os.path.join(f\"statistics/{model_name}/models\", f\"weights_{file_name}\")\n",
    "        \n",
    "        with open(path_sc, 'wb') as f:\n",
    "            pickle.dump(standard_scaler, f)\n",
    "            \n",
    "        model.save_net(path_net)\n",
    "        model.save_model_weights(path_weights)\n",
    "    \n",
    "        # dump statistics to pickle    \n",
    "        file_name += \".pkl\"\n",
    "        path = os.path.join(f\"statistics/{model_name}\", file_name)\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(Statistics, f)\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041424e-f812-4862-a6da-18c6a11ab9df",
   "metadata": {},
   "source": [
    "# Extract the best model for each one of the measurements\n",
    "`c_index` `concordance_td`, `integrated brier score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44989f2-35dd-403e-a53c-66a4a7f99441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "best_statistics = {'concordance_td': -1, \n",
    "                  'ibs': 1,\n",
    "                   'c_index': -1\n",
    "                 }\n",
    "experiments_list = os.listdir(f\"statistics/{model_name}\")\n",
    "for experiment in experiments_list:\n",
    "    if not experiment.startswith(model_name):\n",
    "        continue\n",
    "        \n",
    "    path = os.path.join(f\"statistics/{model_name}\", experiment)\n",
    "    stats = pickle.load(open(path, \"rb\" ))\n",
    "    df = pd.DataFrame(stats)\n",
    "\n",
    "    concordance_td = df[\"avg_concordance_td\"][0]\n",
    "    std_concordance_td = df[\"std_concordance_td\"][0]\n",
    "    ibs = df[\"avg_ibs\"][0]\n",
    "    std_ibs = df[\"std_ibs\"][0]    \n",
    "    c_index = df[\"avg_c_index\"][0]\n",
    "    std_c_index = df[\"std_c_index\"][0]    \n",
    "    \n",
    "    if c_index > best_statistics[\"c_index\"]:\n",
    "        best_statistics[\"c_index\"] = c_index\n",
    "        best_statistics[\"c_index_std\"] = std_c_index\n",
    "        best_statistics[\"c_index_params\"] = experiment\n",
    "    \n",
    "    if concordance_td > best_statistics[\"concordance_td\"]:\n",
    "        best_statistics[\"concordance_td\"] = concordance_td\n",
    "        best_statistics[\"concordance_td_std\"] = std_concordance_td\n",
    "        best_statistics[\"concordance_td_params\"] = experiment\n",
    "        \n",
    "    if ibs < best_statistics[\"ibs\"]:\n",
    "        best_statistics[\"ibs\"] = ibs\n",
    "        best_statistics[\"ibs_std\"] = std_ibs\n",
    "        best_statistics[\"ibs_params\"] = experiment\n",
    "\n",
    "\n",
    "\n",
    "path = os.path.join(f\"statistics/{model_name}\", \"best_model.pkl\")\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(best_statistics, f)    \n",
    "    \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8bdb707-a761-4bc3-95f0-d4e4553f5a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd59ec1-22e2-43b9-8e98-c5243e38c14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa-proj-ker",
   "language": "python",
   "name": "sa-proj-ker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
