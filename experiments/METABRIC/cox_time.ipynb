{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2255be7c-f6cb-451e-b5fd-1db1ea5a2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbec2c38-6d19-421e-a270-d77a130c14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"METABRIC\"\n",
    "train = pd.read_csv(f\"../../datasets/train/{dataset_name}.csv\")\n",
    "test = pd.read_csv(f\"../../datasets/test/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2903a1-16bb-4837-81d6-af2357444ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete instances with duration = 0.0\n",
    "idx = train[train[\"duration\"] == 0].index[0] if (train[\"duration\"] == 0).any() else None\n",
    "if idx:\n",
    "    train.drop(index=idx, inplace=True)\n",
    "idx = test[test[\"duration\"] == 0].index[0] if (test[\"duration\"] == 0).any() else None\n",
    "if idx:\n",
    "    test.drop(index=idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af100904-5dd8-4ccc-9428-d5e705a8442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE (1712, 11), TEST SHAPE (191, 11)\n"
     ]
    }
   ],
   "source": [
    "print(f\"TRAIN SHAPE {train.shape}, TEST SHAPE {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70337be-5d68-41a4-ba5f-b2fe37108b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.drop([\"event\", \"duration\"], axis=1), train[[\"event\", \"duration\"]]\n",
    "X_test, y_test = test.drop([\"event\", \"duration\"], axis=1), test[[\"event\", \"duration\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fac4c66-5047-43b9-8d3c-5b87f70c225a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.166604</td>\n",
       "      <td>5.434787</td>\n",
       "      <td>14.091119</td>\n",
       "      <td>5.868354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.006690</td>\n",
       "      <td>6.515598</td>\n",
       "      <td>9.696476</td>\n",
       "      <td>5.975583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1         x2        x3   x4   x5   x6   x7     x8\n",
       "0  7.166604  5.434787  14.091119  5.868354  0.0  1.0  1.0  0.0  39.53\n",
       "1  6.006690  6.515598   9.696476  5.975583  0.0  0.0  0.0  1.0  48.07"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1850561f-05eb-42fd-bb0b-d312cbd0619b",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning\n",
    "### The `grid_params` dict describe all the possible hyperparameter values i chose to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94aca34f-5e4a-40ae-a416-f35562ac9a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.models import CoxTime\n",
    "from pycox.models.cox_time import MLPVanillaCoxTime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import KFold\n",
    "from pycox.evaluation import EvalSurv\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from torch.nn.modules.activation import ReLU, SELU\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "np.random.seed(42)\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df67b425-285b-4a9f-88b6-a1644c5a34c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:86: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((sub, t), batch_size, True, eval_, num_workers=num_workers)).flatten().sum()\n",
      "/Users/shakedcaspi/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/pycox/models/cox_time.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(self.predict((input, t), batch_size, True, eval_, num_workers=num_workers)).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "CPU times: user 4h 3min 35s, sys: 4h 47min 4s, total: 8h 50min 40s\n",
      "Wall time: 2h 19min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_name = \"cox_time\"\n",
    "\n",
    "grid_params = {\n",
    "    # net params\n",
    "    \"dropout\": [0.2, 0.1],\n",
    "    \"num_nodes\": [[32, 32], [28,28,100,28,28], [32,128,128], [16,32,64,64]],\n",
    "    \"activation\": [ReLU, SELU],\n",
    "    # loss params\n",
    "    \"shrink\": [1e-1, 1e-2],\n",
    "    # fit params\n",
    "    \"batch_size\": [256],\n",
    "    \"epochs\": [128, 256],\n",
    "    # optimizer params\n",
    "    \"lr\": [1e-2, 1e-4],\n",
    "    \"weight_decay\": [1e-2, 1e-3]\n",
    "}\n",
    "\n",
    "keys, values = zip(*grid_params.items())\n",
    "experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "for experiment in experiments:\n",
    "    Statistics = {'concordance_td':[], \n",
    "                  'ibs': [],\n",
    "                  'c_index': [],\n",
    "                  'avg_concordance_td':0.5, \n",
    "                  'avg_ibs': 0,\n",
    "                  'avg_c_index': 0.5,\n",
    "                  'std_concordance_td': 0,\n",
    "                  'std_ibs': 0,\n",
    "                  'std_c_index': 0                  \n",
    "                 }\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for i, (train_index, val_index) in enumerate(cv.split(X)):\n",
    "\n",
    "        # split to train and test \n",
    "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
    "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
    "\n",
    "        # preprocess for y\n",
    "        lower, upper = np.min(y_train[\"duration\"]), np.max(y_train[\"duration\"])\n",
    "        idx = np.where((y_val[\"duration\"] > lower) & (y_val[\"duration\"] < upper))[0]\n",
    "        X_val = X_val.iloc[idx].copy()\n",
    "        y_val = y_val.iloc[idx].copy()\n",
    "\n",
    "        original_y_train = y_train.copy()\n",
    "        original_y_val = y_val.copy()\n",
    "\n",
    "        # preprocess step\n",
    "        cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "        cols_leave = ['x4', 'x5', 'x6', 'x7']\n",
    "\n",
    "        standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "        leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "        standard_scaler = DataFrameMapper(standardize + leave)\n",
    "        X_train = standard_scaler.fit_transform(X_train).astype('float32')\n",
    "        y_train = y_train[\"duration\"].astype('float32').values, y_train[\"event\"].values\n",
    "        \n",
    "        X_val = standard_scaler.transform(X_val).astype('float32')\n",
    "        y_val = y_val[\"duration\"].astype('float32').values, y_val[\"event\"].values\n",
    "        val = tt.tuplefy(X_val, y_val)\n",
    "        \n",
    "        # model params\n",
    "        shrink = experiment[\"shrink\"]\n",
    "        \n",
    "        # build the network\n",
    "        in_features = X_train.shape[1]\n",
    "        num_nodes = experiment[\"num_nodes\"]\n",
    "        dropout = experiment[\"dropout\"]\n",
    "        activation = experiment[\"activation\"]\n",
    "        batch_norm = True\n",
    "        output_bias = False\n",
    "        \n",
    "        net = MLPVanillaCoxTime(in_features, num_nodes, batch_norm, dropout, activation)\n",
    "\n",
    "        optimizer = tt.optim.Adam(lr=experiment[\"lr\"],\n",
    "                                  weight_decay = experiment[\"weight_decay\"])\n",
    "\n",
    "        # fit params\n",
    "        batch_size = experiment[\"batch_size\"]\n",
    "        epochs = experiment[\"epochs\"]\n",
    "        callbacks = [tt.callbacks.EarlyStopping()]\n",
    "        verbose = False\n",
    "\n",
    "        # train the model\n",
    "        model = CoxTime(net, optimizer, shrink=shrink)   \n",
    "        log = model.fit(X_train, y_train, batch_size, epochs, callbacks, verbose,\n",
    "                val_data=val.repeat(10).cat())\n",
    "                \n",
    "        # model evaluation\n",
    "\n",
    "        ## pycox measures\n",
    "        _ = model.compute_baseline_hazards()\n",
    "        estimate_surv = model.predict_surv_df(X_val)\n",
    "        ev = EvalSurv(estimate_surv, y_val[0], y_val[1], censor_surv='km')\n",
    "\n",
    "        tmp_times = np.sort(y_train[0])\n",
    "        times = np.array([tmp_times[i] for i in range(0, len(tmp_times), 10)])\n",
    "\n",
    "        concordance_td = ev.concordance_td('antolini')\n",
    "        ibs = ev.integrated_brier_score(times)\n",
    "\n",
    "        ## lifelines measures\n",
    "        estimate = np.mean(1-estimate_surv, axis=0)\n",
    "        c_index = 1 - concordance_index(event_times=original_y_val[\"duration\"], \n",
    "                          predicted_scores= estimate, \n",
    "                          event_observed=original_y_val[\"event\"])\n",
    "\n",
    "        # store statistics in Statistics dict\n",
    "        Statistics[\"c_index\"].append(c_index)\n",
    "        Statistics[\"ibs\"].append(ibs)\n",
    "        Statistics[\"concordance_td\"].append(concordance_td)\n",
    "        \n",
    "    # summarise cross validation scores\n",
    "    Statistics[\"avg_concordance_td\"] = np.mean(Statistics[\"concordance_td\"])\n",
    "    Statistics[\"avg_ibs\"] = np.mean(Statistics[\"ibs\"])\n",
    "    Statistics[\"avg_c_index\"] = np.mean(Statistics[\"c_index\"])    \n",
    "    Statistics[\"std_concordance_td\"] = np.std(Statistics[\"concordance_td\"])\n",
    "    Statistics[\"std_ibs\"] = np.std(Statistics[\"ibs\"])    \n",
    "    Statistics[\"std_c_index\"] = np.std(Statistics[\"c_index\"])\n",
    "    \n",
    "\n",
    "    # save the model for later\n",
    "    try:\n",
    "        os.mkdir(f\"statistics/{model_name}/models\")\n",
    "\n",
    "    except OSError as error: \n",
    "        \n",
    "        file_name = f\"{model_name}_\"\n",
    "        for k,v in experiment.items():\n",
    "            file_name += f\"{str(k)}_{str(v)}_\"\n",
    "\n",
    "        # dump model to files\n",
    "        path_sc = os.path.join(f\"statistics/{model_name}/models\", f\"sc_{file_name}.pkl\")            \n",
    "        path_net = os.path.join(f\"statistics/{model_name}/models\", f\"net_{file_name}\")\n",
    "        path_weights = os.path.join(f\"statistics/{model_name}/models\", f\"weights_{file_name}\")\n",
    "        \n",
    "        with open(path_sc, 'wb') as f:\n",
    "            pickle.dump(standard_scaler, f)\n",
    "            \n",
    "        model.save_net(path_net)\n",
    "        model.save_model_weights(path_weights)\n",
    "\n",
    "        # dump statistics to pickle    \n",
    "        file_name += \".pkl\"\n",
    "        path = os.path.join(f\"statistics/{model_name}\", file_name)\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(Statistics, f)\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041424e-f812-4862-a6da-18c6a11ab9df",
   "metadata": {},
   "source": [
    "# Extract the best model for each one of the measurements\n",
    "`c_index`, `concordance_td`, `integrated brier score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d44989f2-35dd-403e-a53c-66a4a7f99441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "best_statistics = {'concordance_td': -1, \n",
    "                  'ibs': 1,\n",
    "                   'c_index': -1\n",
    "                 }\n",
    "experiments_list = os.listdir(f\"statistics/{model_name}\")\n",
    "for experiment in experiments_list:\n",
    "    if not experiment.startswith(model_name):\n",
    "        continue\n",
    "        \n",
    "    path = os.path.join(f\"statistics/{model_name}\", experiment)\n",
    "    stats = pickle.load(open(path, \"rb\" ))\n",
    "    df = pd.DataFrame(stats)\n",
    "\n",
    "    concordance_td = df[\"avg_concordance_td\"][0]\n",
    "    std_concordance_td = df[\"std_concordance_td\"][0]\n",
    "    ibs = df[\"avg_ibs\"][0]\n",
    "    std_ibs = df[\"std_ibs\"][0]    \n",
    "    c_index = df[\"avg_c_index\"][0]\n",
    "    std_c_index = df[\"std_c_index\"][0]    \n",
    "    \n",
    "    if c_index > best_statistics[\"c_index\"]:\n",
    "        best_statistics[\"c_index\"] = c_index\n",
    "        best_statistics[\"c_index_std\"] = std_c_index\n",
    "        best_statistics[\"c_index_params\"] = experiment\n",
    "    \n",
    "    if concordance_td > best_statistics[\"concordance_td\"]:\n",
    "        best_statistics[\"concordance_td\"] = concordance_td\n",
    "        best_statistics[\"concordance_td_std\"] = std_concordance_td\n",
    "        best_statistics[\"concordance_td_params\"] = experiment\n",
    "        \n",
    "    if ibs < best_statistics[\"ibs\"]:\n",
    "        best_statistics[\"ibs\"] = ibs\n",
    "        best_statistics[\"ibs_std\"] = std_ibs\n",
    "        best_statistics[\"ibs_params\"] = experiment\n",
    "\n",
    "\n",
    "\n",
    "path = os.path.join(f\"statistics/{model_name}\", \"best_model.pkl\")\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(best_statistics, f)    \n",
    "    \n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdb707-a761-4bc3-95f0-d4e4553f5a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa-proj-ker",
   "language": "python",
   "name": "sa-proj-ker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
