{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa65c78-e6b1-4567-80c7-ffd492fdf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b608a30c-03bf-4590-8bcd-d02020c0de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"METABRIC\"\n",
    "train = pd.read_csv(f\"../../datasets/train/{dataset_name}.csv\")\n",
    "test = pd.read_csv(f\"../../datasets/test/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aef9a24-5928-4f01-a44f-44e767d3bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete instances with duration = 0.0\n",
    "idx = train[train[\"duration\"] == 0].index[0] if (train[\"duration\"] == 0).any() else None\n",
    "if idx:\n",
    "    train.drop(index=idx, inplace=True)\n",
    "idx = test[test[\"duration\"] == 0].index[0] if (test[\"duration\"] == 0).any() else None\n",
    "if idx:\n",
    "    test.drop(index=idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9932026e-945a-4296-a408-55bbdf4144ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPE (1712, 11), TEST SHAPE (191, 11)\n"
     ]
    }
   ],
   "source": [
    "print(f\"TRAIN SHAPE {train.shape}, TEST SHAPE {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fa6100-f406-43f5-b7ed-94387c728aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.drop([\"event\", \"duration\"], axis=1), train[[\"event\", \"duration\"]]\n",
    "X_test, y_test = test.drop([\"event\", \"duration\"], axis=1), test[[\"event\", \"duration\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba42588-3567-4186-8a91-07069668630f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.166604</td>\n",
       "      <td>5.434787</td>\n",
       "      <td>14.091119</td>\n",
       "      <td>5.868354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.006690</td>\n",
       "      <td>6.515598</td>\n",
       "      <td>9.696476</td>\n",
       "      <td>5.975583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1         x2        x3   x4   x5   x6   x7     x8\n",
       "0  7.166604  5.434787  14.091119  5.868354  0.0  1.0  1.0  0.0  39.53\n",
       "1  6.006690  6.515598   9.696476  5.975583  0.0  0.0  0.0  1.0  48.07"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c86197-b52d-4110-b78b-b98a05e8416b",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning\n",
    "### The `grid_params` dict describe all the possible hyperparameter values i chose to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94aca34f-5e4a-40ae-a416-f35562ac9a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from auton_survival.models.dsm import DeepSurvivalMachines\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import KFold\n",
    "from pycox.evaluation import EvalSurv\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "np.random.seed(42)\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df67b425-285b-4a9f-88b6-a1644c5a34c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████▊                                                                                               | 1025/10000 [00:00<00:07, 1216.37it/s]\n",
      " 13%|██████████████▌                                                                                                 | 13/100 [00:00<00:04, 20.73it/s]\n",
      " 10%|██████████▉                                                                                               | 1027/10000 [00:00<00:07, 1220.23it/s]\n",
      " 49%|██████████████████████████████████████████████████████▉                                                         | 49/100 [00:01<00:02, 25.28it/s]\n",
      " 11%|███████████▌                                                                                               | 1080/10000 [00:01<00:09, 982.63it/s]\n",
      "  9%|██████████▏                                                                                                      | 9/100 [00:00<00:04, 20.37it/s]\n",
      " 10%|██████████▍                                                                                                | 970/10000 [00:00<00:07, 1241.76it/s]\n",
      " 34%|██████████████████████████████████████                                                                          | 34/100 [00:01<00:02, 29.31it/s]\n",
      " 10%|██████████▎                                                                                                | 966/10000 [00:00<00:07, 1142.09it/s]\n",
      " 36%|████████████████████████████████████████▎                                                                       | 36/100 [00:01<00:02, 26.58it/s]\n",
      " 10%|██████████▊                                                                                               | 1025/10000 [00:00<00:07, 1180.34it/s]\n",
      "  1%|█▍                                                                                                             | 13/1000 [00:00<00:37, 26.43it/s]\n",
      " 10%|██████████▉                                                                                               | 1027/10000 [00:00<00:07, 1196.58it/s]\n",
      "  5%|█████▍                                                                                                         | 49/1000 [00:01<00:38, 24.68it/s]\n",
      " 11%|███████████▍                                                                                              | 1080/10000 [00:00<00:07, 1234.53it/s]\n",
      "  1%|█                                                                                                               | 9/1000 [00:00<00:45, 21.64it/s]\n",
      " 10%|██████████▍                                                                                                | 970/10000 [00:00<00:07, 1217.58it/s]\n",
      "  3%|███▊                                                                                                           | 34/1000 [00:01<00:34, 28.12it/s]\n",
      " 10%|██████████▎                                                                                                | 966/10000 [00:00<00:07, 1243.62it/s]\n",
      "  4%|███▉                                                                                                           | 36/1000 [00:01<00:34, 27.97it/s]\n",
      "  5%|████▊                                                                                                      | 451/10000 [00:00<00:07, 1294.84it/s]\n",
      "  5%|█████▋                                                                                                           | 5/100 [00:00<00:04, 22.89it/s]\n",
      "  5%|█████▌                                                                                                     | 519/10000 [00:00<00:07, 1324.57it/s]\n",
      " 42%|███████████████████████████████████████████████                                                                 | 42/100 [00:01<00:01, 37.27it/s]\n",
      "  6%|█████▉                                                                                                     | 553/10000 [00:00<00:07, 1294.35it/s]\n",
      "  8%|█████████                                                                                                        | 8/100 [00:00<00:03, 27.27it/s]\n",
      "  4%|████▊                                                                                                      | 450/10000 [00:00<00:07, 1317.04it/s]\n",
      "  7%|███████▉                                                                                                         | 7/100 [00:00<00:03, 25.35it/s]\n",
      "  5%|█████▌                                                                                                     | 515/10000 [00:00<00:07, 1273.10it/s]\n",
      "  8%|█████████                                                                                                        | 8/100 [00:00<00:03, 25.65it/s]\n",
      "  5%|████▊                                                                                                      | 451/10000 [00:00<00:07, 1320.10it/s]\n",
      "  0%|▌                                                                                                               | 5/1000 [00:00<00:49, 20.23it/s]\n",
      "  5%|█████▌                                                                                                     | 519/10000 [00:00<00:07, 1278.39it/s]\n",
      "  4%|████▋                                                                                                          | 42/1000 [00:01<00:27, 34.77it/s]\n",
      "  6%|█████▉                                                                                                     | 553/10000 [00:00<00:07, 1316.60it/s]\n",
      "  1%|▉                                                                                                               | 8/1000 [00:00<00:38, 25.79it/s]\n",
      "  4%|████▊                                                                                                      | 450/10000 [00:00<00:07, 1290.66it/s]\n",
      "  1%|▊                                                                                                               | 7/1000 [00:00<00:43, 22.94it/s]\n",
      "  5%|█████▌                                                                                                     | 515/10000 [00:00<00:07, 1325.55it/s]\n",
      "  1%|▉                                                                                                               | 8/1000 [00:00<00:40, 24.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:07<00:00, 1424.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 33.40it/s]\n",
      "  1%|▋                                                                                                           | 67/10000 [00:00<00:07, 1261.27it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "value cannot be converted to type float without overflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:84\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/auton-survival/auton_survival/models/dsm/__init__.py:260\u001b[0m, in \u001b[0;36mDSMBase.fit\u001b[0;34m(self, x, t, e, vsize, val_data, iters, learning_rate, batch_size, elbo, optimizer)\u001b[0m\n\u001b[1;32m    258\u001b[0m maxrisk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mnanmax(e_train\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m    259\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_torch_model(inputdim, optimizer, risks\u001b[38;5;241m=\u001b[39mmaxrisk)\n\u001b[0;32m--> 260\u001b[0m model, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dsm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                     \u001b[49m\u001b[43melbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melbo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sa-proj/lib/python3.9/site-packages/auton-survival/auton_survival/models/dsm/utilities.py:142\u001b[0m, in \u001b[0;36mtrain_dsm\u001b[0;34m(model, x_train, t_train, e_train, x_valid, t_valid, e_valid, n_iter, lr, elbo, bs, random_seed)\u001b[0m\n\u001b[1;32m    132\u001b[0m premodel \u001b[38;5;241m=\u001b[39m pretrain_dsm(model,\n\u001b[1;32m    133\u001b[0m                         t_train_,\n\u001b[1;32m    134\u001b[0m                         e_train_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m                         lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m,\n\u001b[1;32m    139\u001b[0m                         thres\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(model\u001b[38;5;241m.\u001b[39mrisks):\n\u001b[0;32m--> 142\u001b[0m   \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpremodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   model\u001b[38;5;241m.\u001b[39mscale[\u001b[38;5;28mstr\u001b[39m(r\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;28mfloat\u001b[39m(premodel\u001b[38;5;241m.\u001b[39mscale[\u001b[38;5;28mstr\u001b[39m(r\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]))\n\u001b[1;32m    145\u001b[0m model\u001b[38;5;241m.\u001b[39mdouble()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: value cannot be converted to type float without overflow"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_name = \"dsm\"\n",
    "\n",
    "grid_params = {\n",
    "    # net params\n",
    "    \"num_nodes\": [[41], [32, 32], [28,28,100,28,28], [32,128,128], [16,32,64,64]],\n",
    "    \"k\": [3,4],\n",
    "    \"distribution\" : [\"Weibull\"],        \n",
    "    # fit params\n",
    "    \"batch_size\": [256, 512],\n",
    "    \"epochs\": [256, 512],\n",
    "    # optimizer params\n",
    "    \"lr\": [1e-2, 1e-3],\n",
    "    \"optimizer\": [\"Adam\", \"RMSProp\", \"SGD\"],\n",
    "    \"iters\": [100,1000]        \n",
    "}\n",
    "\n",
    "keys, values = zip(*grid_params.items())\n",
    "experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "for experiment in experiments:\n",
    "    Statistics = {'concordance_td':[], \n",
    "                  'ibs': [],\n",
    "                  'c_index': [],\n",
    "                  'avg_concordance_td':0.5, \n",
    "                  'avg_ibs': 0,\n",
    "                  'avg_c_index': 0.5,\n",
    "                  'std_concordance_td': 0,\n",
    "                  'std_ibs': 0,\n",
    "                  'std_c_index': 0                  \n",
    "                 }\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for i, (train_index, val_index) in enumerate(cv.split(X)):\n",
    "\n",
    "        # split to train and test \n",
    "        X_train, X_val = X.iloc[train_index].copy(), X.iloc[val_index].copy()\n",
    "        y_train, y_val = y.iloc[train_index].copy(), y.iloc[val_index].copy()\n",
    "\n",
    "        # preprocess for y\n",
    "        lower, upper = np.min(y_train[\"duration\"]), np.max(y_train[\"duration\"])\n",
    "        idx = np.where((y_val[\"duration\"] > lower) & (y_val[\"duration\"] < upper))[0]\n",
    "        X_val = X_val.iloc[idx].copy()\n",
    "        y_val = y_val.iloc[idx].copy()\n",
    "\n",
    "        y_train = y_train.astype('float32')\n",
    "        y_val = y_val.astype('float32')\n",
    "        \n",
    "        original_y_train = y_train.copy()\n",
    "        original_y_val = y_val.copy()\n",
    "        \n",
    "        # preprocess step\n",
    "        cols_standardize = ['x0', 'x1', 'x2', 'x3', 'x8']\n",
    "        cols_leave = ['x4', 'x5', 'x6', 'x7']\n",
    "\n",
    "        standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "        leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "        standard_scaler = DataFrameMapper(standardize + leave)\n",
    "        X_train = standard_scaler.fit_transform(X_train).astype('float32')\n",
    "        y_train = y_train[\"duration\"].values, y_train[\"event\"].values\n",
    "\n",
    "        X_val = standard_scaler.transform(X_val).astype('float32')\n",
    "        y_val = y_val[\"duration\"].values, y_val[\"event\"].values\n",
    "        val = X_val, y_val[0], y_val[1]\n",
    "\n",
    "\n",
    "        \n",
    "        # build the network\n",
    "        num_nodes = experiment[\"num_nodes\"]\n",
    "        k = experiment[\"k\"]\n",
    "        distribution = experiment[\"distribution\"]\n",
    "        use_activation = True\n",
    "        \n",
    "        # fit params\n",
    "        lr = experiment[\"lr\"]        \n",
    "        optimizer = experiment[\"optimizer\"]        \n",
    "        batch_size = experiment[\"batch_size\"]\n",
    "        iters = experiment[\"iters\"]\n",
    "        \n",
    "\n",
    "        # train the model\n",
    "        model = DeepSurvivalMachines(k=k, layers=num_nodes, \n",
    "                                     distribution=distribution, random_seed=42)\n",
    "        \n",
    "        model.fit(x = X_train, t = y_train[0], e = y_train[1], \n",
    "                  val_data = val, iters=iters, learning_rate=lr, \n",
    "                  batch_size=batch_size, optimizer=optimizer)\n",
    "\n",
    "        # model evaluation\n",
    "        \n",
    "        ## pycox measures\n",
    "        tmp_times = np.sort(y_train[0])\n",
    "        times = [tmp_times[i] for i in range(0, len(tmp_times), 10)]\n",
    "        \n",
    "        estimate_surv = model.predict_survival(X_val, times)\n",
    "        estimate_surv = pd.DataFrame(estimate_surv.T)\n",
    "        ev = EvalSurv(estimate_surv, y_val[0], y_val[1], censor_surv='km')\n",
    "        \n",
    "        concordance_td = ev.concordance_td('antolini')\n",
    "        ibs = ev.integrated_brier_score(np.array(times))\n",
    "\n",
    "        ## lifelines measures\n",
    "        estimate = np.mean(1-estimate_surv.to_numpy(), axis=0)\n",
    "        c_index = 1 - concordance_index(event_times=original_y_val[\"duration\"], \n",
    "                          predicted_scores= estimate, \n",
    "                          event_observed=original_y_val[\"event\"])\n",
    "\n",
    "        # store statistics in Statistics dict\n",
    "        Statistics[\"c_index\"].append(c_index)\n",
    "        Statistics[\"ibs\"].append(ibs)\n",
    "        Statistics[\"concordance_td\"].append(concordance_td)\n",
    "\n",
    "        \n",
    "    # summarise cross validation scores\n",
    "    Statistics[\"avg_concordance_td\"] = np.mean(Statistics[\"concordance_td\"])\n",
    "    Statistics[\"avg_ibs\"] = np.mean(Statistics[\"ibs\"])\n",
    "    Statistics[\"avg_c_index\"] = np.mean(Statistics[\"c_index\"])    \n",
    "    Statistics[\"std_concordance_td\"] = np.std(Statistics[\"concordance_td\"])\n",
    "    Statistics[\"std_ibs\"] = np.std(Statistics[\"ibs\"])    \n",
    "    Statistics[\"std_c_index\"] = np.std(Statistics[\"c_index\"])\n",
    "    \n",
    "    # save the model for later\n",
    "    try:\n",
    "        os.mkdir(f\"statistics/{model_name}/models\")\n",
    "\n",
    "    except OSError as error: \n",
    "        \n",
    "        file_name = f\"{model_name}_\"\n",
    "        for k,v in experiment.items():\n",
    "            file_name += f\"{str(k)}_{str(v)}_\"\n",
    "        \n",
    "        # dump model to files\n",
    "        path_sc = os.path.join(f\"statistics/{model_name}/models\", f\"sc_{file_name}.pkl\")            \n",
    "        path_model = os.path.join(f\"statistics/{model_name}/models\", f\"model_{file_name}.pt\")\n",
    "        \n",
    "        with open(path_sc, 'wb') as f:\n",
    "            pickle.dump(standard_scaler, f)\n",
    "            \n",
    "        torch.save(model.torch_model.state_dict(), path_model)\n",
    "    \n",
    "        # dump statistics to pickle    \n",
    "        file_name += \".pkl\"\n",
    "        path = os.path.join(f\"statistics/{model_name}\", file_name)\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(Statistics, f)\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901177e9-8fa5-4521-8316-33509e751bab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract the best model for each one of the measurements\n",
    "`c_index` `concordance_td`, `integrated brier score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44989f2-35dd-403e-a53c-66a4a7f99441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "best_statistics = {'concordance_td': -1, \n",
    "                  'ibs': 1,\n",
    "                   'c_index': -1\n",
    "                 }\n",
    "experiments_list = os.listdir(f\"statistics/{model_name}\")\n",
    "for experiment in experiments_list:\n",
    "    if not experiment.startswith(model_name):\n",
    "        continue\n",
    "        \n",
    "    path = os.path.join(f\"statistics/{model_name}\", experiment)\n",
    "    stats = pickle.load(open(path, \"rb\" ))\n",
    "    df = pd.DataFrame(stats)\n",
    "\n",
    "    concordance_td = df[\"avg_concordance_td\"][0]\n",
    "    std_concordance_td = df[\"std_concordance_td\"][0]\n",
    "    ibs = df[\"avg_ibs\"][0]\n",
    "    std_ibs = df[\"std_ibs\"][0]    \n",
    "    c_index = df[\"avg_c_index\"][0]\n",
    "    std_c_index = df[\"std_c_index\"][0]    \n",
    "    \n",
    "    if c_index > best_statistics[\"c_index\"]:\n",
    "        best_statistics[\"c_index\"] = c_index\n",
    "        best_statistics[\"c_index_std\"] = std_c_index\n",
    "        best_statistics[\"c_index_std\"] = experiment\n",
    "    \n",
    "    if concordance_td > best_statistics[\"concordance_td\"]:\n",
    "        best_statistics[\"concordance_td\"] = concordance_td\n",
    "        best_statistics[\"concordance_td_std\"] = std_concordance_td\n",
    "        best_statistics[\"concordance_td_params\"] = experiment\n",
    "        \n",
    "    if ibs < best_statistics[\"ibs\"]:\n",
    "        best_statistics[\"ibs\"] = ibs\n",
    "        best_statistics[\"ibs_std\"] = std_ibs\n",
    "        best_statistics[\"ibs_params\"] = experiment\n",
    "\n",
    "\n",
    "\n",
    "path = os.path.join(f\"statistics/{model_name}\", \"best_model.pkl\")\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(best_statistics, f)    \n",
    "    \n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa-proj-ker",
   "language": "python",
   "name": "sa-proj-ker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
